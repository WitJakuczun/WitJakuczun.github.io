[{"content":"Introducing Hamilton Hamilton is the general purpose micro-framework for creating dataflows from python functions!\nIts selling points are:\n very light (as oposed to Kedro) declarative easy to debug allows lineage basic type checking based on type hints easy to version (just use git) easy to test \u0026amp; debug flows can be parameterized (e.g. different hyperparameters for different models) flows can have optional nodes (e.g. debug on/off) support DRY principle when you write many functions with the same logic but different parameters.  Its drawbacks are:\n No caching (???) which means each time you have to rerun everything. Kedro for example has caching.  My comment Use it as feature store Hamilton was created to solve feature calculation.\nIt looks like a feature store for poor. It can help you manage your features calculation for your dataframes. For sure it can be very useful for column-based features, like day of week or lagged vars.\nTo be checked if it can help with feature transformations that learn from train data. As Hamilton is general enough it should work.\nUse it as etl Hamilton is micro-framework which means it does not solve orchestrating problem. Practically we have a tool that can be embedded into any workflow management solution like Airflow or Kubeflow.\nHamilton is general enough to work on any python objects. This means you can create advanced ETL code (joins, groupbys, etc.) in an organized way.\nUse it as a general dataflow for training/predicting models As with feature store it can be seen as Kedro or Kubeflow for poor.\nIt is a good tool to create generic training function for machine learning model with feature transformations and model fitting. An example of such flow can be found here.\nCan be scaled It looks thatHamilton can be scaled with Dask, Ray or Spark.\nProbably useful for huge number of features.\n","permalink":"https://WitJakuczun.gihub.io/posts/hamilton-framework-for-creating-dataflows-from-python-funcitons/","summary":"Thoughts on \u003ca href=\"https://github.com/stitchfix/hamilton\"\u003eHamilton\u003c/a\u003e framework from Stichfix.","title":"Hamilton Framework for Creating Dataflows From Python Funcitons"},{"content":"Installation /bin/bash -c \u0026#34;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\u0026#34; echo \u0026#39;eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34;\u0026#39; \u0026gt;\u0026gt; /home/wit/.profile eval \u0026#34;$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\u0026#34; Using homebrew You can use homebrew in the same way as under MacOS. Example below is an instruction to install hugo.\nbrew install hugo ","permalink":"https://WitJakuczun.gihub.io/posts/homebrew-for-linux/","summary":"Using Homebrew with Linux","title":"Homebrew for Linux"},{"content":"Use case description I treat Kedro pipelines as pure as possible. For example if I build ML system with Kedro I create train pipeline that takes a dataset as an input (e.g. CSV or Parquet file) and using hyperparameters as inputs (mostly brought as script args) train a model that is stored a binary artifact (also represented as Kedro\u0026rsquo;s dataset).\nWhat is a challenge here? In Kedro datasets are defined in a catalog. They have names that are used in pipelines as in example below:\npipeline([ node( func=train, inputs=[\u0026#34;train\u0026#34;, \u0026#34;params\u0026#34;], outputs=\u0026#34;model\u0026#34;, name=\u0026#34;train_model\u0026#34; )]) In the example I create a pipeline that call train function and has two inputs:\n train dataset - a Kedro\u0026rsquo;s dataset named train params - parameters taken from parameters.yml augmented by script\u0026rsquo;s args  The function has one output that is Kedro\u0026rsquo;s dataset called model.\nThe beauty of Kedro is that I fully abstract what is representation of inputs and outputs. It is handled by Kedro transparently. So having such function I would like to run the pipeline for different train datasets for example to calculate CV-K model`s performance.\nAugmenting Kedro\u0026rsquo;s catalog with hooks Using ProjectContext class Kedro provides two ways to augment DataCatalog. First is to derive custom class (create your own _get_catalog function) from ProjectContext class and edit settings.py so that CONTEXT_CLASS points to it.\nUsing hooks Second option is to use hooks. I choose before_pipeline_run hook because at this stage both catalog is ready and I have scripts args provided (as run_params).\nWhat did I do? I created hooks.py file in which I created a class MyHooks. Next in this class I created a method before_pipeline_run decorated with @hook_impl. Example below should be clear enough (function create_catalog does the magic.)\nclass MyHooks: @property def _logger(self): return logging.getLogger(self.__class__.__name__) @hook_impl def before_pipeline_run(self, run_params:Dict[str, Any], pipeline:Pipeline, catalog: DataCatalog) -\u0026gt; None: config_loader = ConfigLoader(\u0026#34;conf\u0026#34;) credentials = config_loader.get(\u0026#34;credentials.yml\u0026#34;)[\u0026#34;dev_s3\u0026#34;] self._logger.info(run_params) create_catalog(catalog, run_params[\u0026#34;extra_params\u0026#34;][\u0026#34;run\u0026#34;], credentials) Where function create_catalog looks like this\ndef create_catalog(catalog:DataCatalog, run_params:Dict[str, Any], credentials:Dict[str, Any]) -\u0026gt; DataCatalog: from kedro.extras.datasets.pandas import CSVDataSet from kedro.extras.datasets.pickle.pickle_dataset import PickleDataSet base_data_path = \u0026#34;s3://mybucket\u0026#34; train_name = \u0026#34;train\u0026#34; test_name = \u0026#34;test\u0026#34; catalog.add(data_set_name=train_name, data_set=CSVDataSet( filepath=f\u0026#34;{base_data_path}/folds/{run_params[\u0026#39;train\u0026#39;]}.csv\u0026#34;, load_args={\u0026#34;sep\u0026#34;:\u0026#34;,\u0026#34;}, credentials=credentials)) As you can see I simple use add method of DataCatalog class to create new entries with given names (e.g. train) and I augment physical file path using scripts\u0026rsquo; args b.\n","permalink":"https://WitJakuczun.gihub.io/posts/tweaking-kedro-catalog-with-hooks-and-runtime-script-args/","summary":"How to augment Kedro\u0026rsquo;s DataCatalog using runtime script args?\u0026quot;","title":"Tweaking Kedro's DataCatalog With Hooks and Runtime Script Args"},{"content":"Use case description You want to build a forecasting model for each SKU in your data base. Number of SKUs vary. Some can be removed, some can be added. You must query db to check list of active SKUs.\nForecasting models for each SKU are of the same structure. They differ only by the data their paramers are trained on.\nThis can be depicted in the pseudo python lang example as below\ndef build_model(dataset): # do model building... return model for dataset in all_skus_datasets: build_model(dataset) What are design patterns with Kedro? Kedro supports reproducibility. This means that any dynamic pipeline generation based on data is not recommended.\nUsing kedro parameterised runs In this repo there is an example how you can implement parameterised runs in Kedro. This is based on hooks feature of kedro.\nThe trick is you create pipeline for a single SKU. Then you prepare either datasets of inputs (like SKU ids) that are next passed as args into the pipeline. Then you call the whole pipeline in a loop (or in parallel) with different args.\nBut why so complex (e.g using hooks) approach? Kedro pipeline is hardcodded to data catalog. The proposed solution deals with that by dynamically changing catalog using hooks.\nPros  Clear reusable pipeline design  Cons  Processing parallelization must be done with external tool.  ","permalink":"https://WitJakuczun.gihub.io/posts/running-kedro-pipelines-in-parallel-for-different-datasets-or-params/","summary":"How to run kedro pipeline on different  partitions of a dataset. For example build forecasting models for each SKU.","title":"Running Kedro Pipelines in Parallel for Different Datasets or Params"},{"content":"It is a survey article on hyperparameter optimization for ML models. It is available here\n","permalink":"https://WitJakuczun.gihub.io/posts/article-hyperparameter-optimization-foundations-algorithms-best-practices-and-open-challenges/","summary":"Survey article on hyperparameter optimization for ML","title":"Article   Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges"},{"content":"What is Feathr Feathr was developed by LinkedIn and opensourced on April 12, 2022\nFrom github project page we can read that\n Feathr â€“ An Enterprise-Grade, High Performance Feature Store\nFeathr lets you: Define features based on raw data sources, including time-series data, using simple APIs. Get those features by their names during model training and model inferencing. Share features across your team and company.\nFeathr automatically computes your feature values and joins them to your training data, using point-in-time-correct semantics to avoid data leakage, and supports materializing and deploying your features for use online in production.\n My thoughts \u0026hellip;\n","permalink":"https://WitJakuczun.gihub.io/posts/feathr-feature-store/","summary":"Exploring feathr feature store","title":"Feathr Feature Store"},{"content":"Go to Preferences \u0026gt; Keyboard and map Control to Globe and Globe to Control.\n","permalink":"https://WitJakuczun.gihub.io/posts/maos-with-logi-mx-keys-mini/","summary":"How to switch fn with left control with logitech mx keys mini under macos.","title":"MacOs with Logi MX Keys Mini"},{"content":"Installed packages I have installed following brew Packages\ntap \u0026#34;homebrew/bundle\u0026#34; tap \u0026#34;homebrew/cask\u0026#34; tap \u0026#34;homebrew/cask-fonts\u0026#34; tap \u0026#34;homebrew/cask-versions\u0026#34; tap \u0026#34;homebrew/core\u0026#34; brew \u0026#34;cmake\u0026#34; brew \u0026#34;htop\u0026#34; brew \u0026#34;hugo\u0026#34; brew \u0026#34;libomp\u0026#34; brew \u0026#34;llvm@11\u0026#34; brew \u0026#34;midnight-commander\u0026#34; brew \u0026#34;pyenv\u0026#34; brew \u0026#34;wget\u0026#34; cask \u0026#34;amethyst\u0026#34; cask \u0026#34;font-fira-code\u0026#34; cask \u0026#34;google-chrome-dev\u0026#34; cask \u0026#34;iterm2\u0026#34; cask \u0026#34;julia\u0026#34; cask \u0026#34;miniconda\u0026#34; cask \u0026#34;minizincide\u0026#34; cask \u0026#34;obs\u0026#34; cask \u0026#34;remarkable\u0026#34; cask \u0026#34;signal\u0026#34; cask \u0026#34;spotify\u0026#34; cask \u0026#34;visual-studio-code-insiders\u0026#34; cask \u0026#34;vlc\u0026#34; cask \u0026#34;zoom\u0026#34; Restore installed packages Copy installed packages into Brewfile file. Run following command in a directory containing the file.\nbrew bundle ","permalink":"https://WitJakuczun.gihub.io/posts/my-brew-packages/","summary":"How to backup and restore brew packages in your macos.","title":"My Brew Packages"},{"content":"Instalation brew install amethyst ","permalink":"https://WitJakuczun.gihub.io/posts/macos-tile-managment-with-amethyst/","summary":"Making MacOS productive with amethyst tile windows manager","title":"MacOS Tile Managment With Amethyst"},{"content":"General remarks  Failed to use micromamba Was successful with miniconda  Instalation procedure miniconda brew install miniconda Create environment We will create global environment called tf2m1\nconda create -n tf2m1 Activate environment conda env update -n tf2m1 --file requirements.yml Test if everything works Run following code\nconda activate tf2m1 python -c \u0026#34;import tensorflow as tf; print(f\\\u0026#34;TensorFlow has access to the following devices:\\\\n\\\u0026#34;); [print(d) for d in tf.config.list_physical_devices()]\u0026#34; You should see the following output (or similar)\nTensorFlow has access to the following devices: PhysicalDevice(name=\u0026#39;/physical_device:CPU:0\u0026#39;, device_type=\u0026#39;CPU\u0026#39;) PhysicalDevice(name=\u0026#39;/physical_device:GPU:0\u0026#39;, device_type=\u0026#39;GPU\u0026#39;) ","permalink":"https://WitJakuczun.gihub.io/posts/configure-tensorflow-for-m1-pro/","summary":"How to configure TensorFlow to work with M1 Pro","title":"Configure Tensorflow for M1 Pro"}]